package grpc

import (
	"context"
	"crypto/tls"
	"dex-indexer-sol/internal/consts"
	"dex-indexer-sol/internal/svc"
	"errors"
	"fmt"
	"google.golang.org/grpc/metadata"
	"io"
	"log"
	"sync"
	"time"

	pb "github.com/rpcpool/yellowstone-grpc/examples/golang/proto"
	"google.golang.org/grpc"
	"google.golang.org/grpc/credentials"
	"google.golang.org/grpc/keepalive"
)

type GrpcStreamManager struct {
	mu                    sync.Mutex                    // äº’æ–¥é”ï¼Œä¿æŠ¤å¹¶å‘å®‰å…¨
	conn                  *grpc.ClientConn              // gRPC è¿æ¥å¯¹è±¡
	client                pb.GeyserClient               // gRPC å®¢æˆ·ç«¯
	stream                pb.Geyser_SubscribeClient     // gRPC è®¢é˜…æµ
	stopped               bool                          // æ ‡è®°æ˜¯å¦å·²ç»åœæ­¢
	reconnectAttempts     int                           // å·²é‡è¿æ¬¡æ•°
	reconnectInterval     time.Duration                 // é‡è¿åŸºç¡€é—´éš”
	xToken                string                        // è®¤è¯ç”¨çš„ x-token
	streamPingIntervalSec int                           // Streamå¿ƒè·³åŒ…å‘é€é—´éš”ï¼ˆç§’ï¼‰
	blockChan             chan *pb.SubscribeUpdateBlock // åŒºå—æ•°æ®é€šé“
	connCtx               context.Context               // å½“å‰è¿æ¥çš„ context
	connCancel            context.CancelFunc            // å½“å‰è¿æ¥çš„ cancel å‡½æ•°
	recvTimeoutSec        int                           // Recv è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
	blockRecvTimeoutSec   int                           // blockæ¥æ”¶è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
	sendTimeoutSec        int                           // gRPCå‘é€è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
}

func NewGrpcStreamManager(sc *svc.GrpcServiceContext, blockChan chan *pb.SubscribeUpdateBlock) (*GrpcStreamManager, error) {
	grpcConf := sc.Config.Grpc

	configTls := &tls.Config{
		InsecureSkipVerify: true,
	}

	dialCtx, cancel := context.WithTimeout(context.Background(), time.Duration(grpcConf.ConnectTimeoutSec)*time.Second)
	defer cancel()

	conn, err := grpc.DialContext(
		dialCtx,
		grpcConf.Endpoint,
		grpc.WithTransportCredentials(credentials.NewTLS(configTls)),
		grpc.WithInitialWindowSize(int32(grpcConf.InitialWindowSize)),
		grpc.WithInitialConnWindowSize(int32(grpcConf.InitialConnWindowSize)),
		grpc.WithDefaultCallOptions(
			grpc.MaxCallSendMsgSize(grpcConf.MaxCallSendMsgSize),
			grpc.MaxCallRecvMsgSize(grpcConf.MaxCallRecvMsgSize),
		),
		grpc.WithBlock(),
		grpc.WithKeepaliveParams(keepalive.ClientParameters{
			Time:                time.Duration(grpcConf.KeepalivePingIntervalSec) * time.Second,
			Timeout:             time.Duration(grpcConf.KeepalivePingTimeoutSec) * time.Second,
			PermitWithoutStream: true,
		}),
	)
	if err != nil {
		return nil, fmt.Errorf("failed to connect: %v", err)
	}

	return &GrpcStreamManager{
		conn:                  conn,
		client:                pb.NewGeyserClient(conn),
		reconnectAttempts:     0,
		reconnectInterval:     time.Duration(grpcConf.ReconnectIntervalSec) * time.Second,
		xToken:                grpcConf.XToken,
		streamPingIntervalSec: grpcConf.StreamPingIntervalSec,
		blockChan:             blockChan,
		recvTimeoutSec:        grpcConf.RecvTimeoutSec,
		blockRecvTimeoutSec:   grpcConf.BlockRecvTimeoutSec,
		sendTimeoutSec:        grpcConf.SendTimeoutSec,
	}, nil
}

func (m *GrpcStreamManager) Start() {
	m.mustConnect()
}

func (m *GrpcStreamManager) Stop() {
	m.mu.Lock()
	defer m.mu.Unlock()

	m.stopped = true // æ ‡è®°å·²åœæ­¢
	if m.connCancel != nil {
		m.connCancel() // ğŸ”¥ å»ºè®®åŠ ä¸Š
		m.connCancel = nil
	}
	if m.conn != nil {
		err := m.conn.Close()
		_ = err
	}
}

// å†…éƒ¨å¾ªç¯ç›´åˆ°è¿æ¥æˆåŠŸ
func (m *GrpcStreamManager) mustConnect() {
	for {
		m.mu.Lock()
		if m.stopped {
			m.mu.Unlock()
			return
		}
		m.mu.Unlock()

		if m.reconnectAttempts > 0 {
			if m.reconnectAttempts > 3 {
				time.Sleep(m.reconnectInterval * 2)
			} else {
				time.Sleep(m.reconnectInterval)
			}
		}
		log.Printf("Connecting... Attempt %d", m.reconnectAttempts+1)
		m.reconnectAttempts++
		err := m.connect()
		if err == nil {
			return // è¿æ¥æˆåŠŸ
		}
		log.Printf("Connect failed: %v, will retry...", err)
	}
}

func buildSubscribeRequest() *pb.SubscribeRequest {
	blocks := make(map[string]*pb.SubscribeRequestFilterBlocks)
	blocks["blocks"] = &pb.SubscribeRequestFilterBlocks{
		AccountInclude:      consts.GrpcAccountInclude,
		IncludeTransactions: boolPtr(true),  // âœ… ä¿ç•™è½¬ SOLã€swapã€transfer ç­‰äº¤æ˜“
		IncludeAccounts:     boolPtr(false), // ä¸å†æ”¶è´¦æˆ·ä½™é¢å˜åŒ–çš„å•ç‹¬ AccountUpdateï¼ˆvote çœäº†ï¼‰
		IncludeEntries:      boolPtr(false), // IncludeEntries æ˜¯ Solana åº•å±‚çš„æ—¥å¿—ï¼Œæ™®é€šä¸šåŠ¡åŸºæœ¬æ²¡ç”¨ã€‚
	}
	commitment := pb.CommitmentLevel_CONFIRMED
	return &pb.SubscribeRequest{
		Blocks:     blocks,
		Commitment: &commitment,
	}
}

// connect åªå°è¯•ä¸€æ¬¡è¿æ¥
func (m *GrpcStreamManager) connect() error {
	m.mu.Lock()
	if m.stopped {
		m.mu.Unlock()
		return errors.New("manager is stopped")
	}
	defer m.mu.Unlock()

	// å…ˆå…³é—­æ—§çš„ contextï¼Œä¼˜é›…é€€å‡ºæ—§ goroutine
	if m.connCancel != nil {
		m.connCancel()
		m.connCancel = nil
	}
	m.connCtx, m.connCancel = context.WithCancel(context.Background())

	log.Println("Attempting to connect...")

	metaCtx := metadata.NewOutgoingContext(
		m.connCtx,
		metadata.New(map[string]string{"x-token": m.xToken}),
	)
	stream, err := m.client.Subscribe(metaCtx)
	if err != nil {
		log.Printf("Failed to subscribe: %v", err)
		return err // åªè¿”å›é”™è¯¯
	}

	req := buildSubscribeRequest()
	err = sendWithTimeout(m.connCtx, stream.Send, req, time.Duration(m.sendTimeoutSec)*time.Second)
	if err != nil {
		log.Printf("Failed to send request: %v", err)
		return err // åªè¿”å›é”™è¯¯
	}

	m.stream = stream
	m.reconnectAttempts = 0
	log.Println("Connection established")

	// å¯åŠ¨ ping åç¨‹
	go m.pingLoop(m.connCtx)
	// å¯åŠ¨ block ç›‘å¬åç¨‹
	go m.blockRecvLoop(m.connCtx)

	return nil
}

func (m *GrpcStreamManager) blockRecvLoop(ctx context.Context) {
	last := time.Now()
	blockTimeout := time.Duration(m.blockRecvTimeoutSec) * time.Second
	for {
		select {
		case <-ctx.Done():
			return // ä¼˜é›…é€€å‡º
		default:
			update, err := m.stream.Recv() //recvWithTimeout[*pb.SubscribeUpdate](ctx, m.stream.Recv, time.Duration(m.recvTimeoutSec)*time.Second)
			now := time.Now()
			if err != nil {
				if errors.Is(err, io.EOF) {
					log.Printf("Stream closed by server (EOF), will reconnect")
					m.reconnect()
					return
				}

				log.Printf("Stream error: %v", err)
				if m.reconnectIfBlockTimeout(last, blockTimeout) {
					return
				}
				time.Sleep(100 * time.Millisecond)
				continue
			}

			switch u := (*update).GetUpdateOneof().(type) {
			case *pb.SubscribeUpdate_Block:
				interval := now.UnixMilli() - u.Block.BlockTime.Timestamp*1000 // ç®—å‡ºä½ æ”¶åˆ°è¿™ä¸ªåŒºå—æ—¶çš„å»¶è¿Ÿï¼ˆmsï¼‰
				log.Printf("received block at slot %v, latency to blockTime: %v ms", u.Block.Slot, interval)

				//select {
				//case m.blockChan <- u.Block:
				//	// æˆåŠŸå†™å…¥ï¼Œæ— äº‹å‘ç”Ÿ
				//default:
				//	log.Printf("blockChan is full, discard block at slot %v", u.Block.Slot)
				//}
				//interval1 := now.Sub(last)
				//log.Printf("received block at slot %v, interval since last block: %v ms", u.Block.Slot, interval1.Milliseconds())
				// æ— è®ºæ˜¯å¦å†™å…¥æˆåŠŸï¼Œéƒ½è¦æ›´æ–° last
				last = now
			}
		}

		if m.reconnectIfBlockTimeout(last, blockTimeout) {
			return
		}
	}
}

// å¸¦è¶…æ—¶çš„ Send
func sendWithTimeout[T any](ctx context.Context, sendFunc func(T) error, req T, timeout time.Duration) error {
	timeoutCtx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()

	done := make(chan error, 1)
	go func() {
		done <- sendFunc(req)
	}()

	select {
	case <-timeoutCtx.Done():
		return timeoutCtx.Err()
	case err := <-done:
		return err
	}
}

// å¸¦è¶…æ—¶çš„ Recv
func recvWithTimeout[T any](ctx context.Context, recvFunc func() (T, error), timeout time.Duration) (T, error) {
	timeoutCtx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()

	done := make(chan struct {
		resp T
		err  error
	}, 1)

	go func() {
		resp, err := recvFunc()
		done <- struct {
			resp T
			err  error
		}{resp, err}
	}()

	select {
	case <-timeoutCtx.Done():
		var zero T
		return zero, timeoutCtx.Err()
	case result := <-done:
		return result.resp, result.err
	}
}

// å¿ƒè·³æ£€æµ‹
func (m *GrpcStreamManager) pingLoop(ctx context.Context) {
	ticker := time.NewTicker(time.Duration(m.streamPingIntervalSec) * time.Second)
	defer ticker.Stop()
	for {
		select {
		case <-ctx.Done():
			return // ä¼˜é›…é€€å‡º
		case <-ticker.C:
			pingReq := &pb.SubscribeRequest{
				Ping: &pb.SubscribeRequestPing{Id: 1},
			}
			err := sendWithTimeout(ctx, m.stream.Send, pingReq, time.Duration(m.sendTimeoutSec)*time.Second)
			if err != nil {
				log.Printf("Ping failed: %v", err)
				// è¿™é‡Œåªè®°å½•æ—¥å¿—ï¼Œä¸è§¦å‘é‡è¿
			}
		}
	}
}

func (m *GrpcStreamManager) reconnectIfBlockTimeout(last time.Time, timeout time.Duration) bool {
	if time.Since(last) > timeout {
		log.Printf("%væœªæ”¶åˆ°blockï¼Œè§¦å‘é‡è¿", timeout)
		m.reconnect()
		return true
	}
	return false
}

func (m *GrpcStreamManager) reconnect() {
	m.mu.Lock()
	if m.stopped {
		m.mu.Unlock()
		return
	}
	if m.connCancel != nil {
		m.connCancel() // å…³é—­æ‰€æœ‰ç›¸å…³ goroutine
		m.connCancel = nil
	}
	m.mu.Unlock()

	go m.mustConnect()
}

func boolPtr(b bool) *bool {
	return &b
}

// main å‡½æ•°å·²æ³¨é‡Šï¼Œæ”¹ä¸ºæä¾› Newã€Startã€Stop æ¥å£
/*
func main() {
	ctx := context.Background()

	// Create manager with data handler and x-token
	manager, err := NewGrpcStreamManager(
		"your-grpc-url:2053",
		"your-x-token",
		handleAccountUpdate,
		10,
		1<<30,
		1<<30,
		64*1024*1024,
		64*1024*1024,
	)
	if err != nil {
		log.Fatal(err)
	}
	defer manager.Close()

	// Create subscription request for blocks and slots
	blocks := make(map[string]*pb.SubscribeRequestFilterBlocks)
	blocks["blocks"] = &pb.SubscribeRequestFilterBlocks{
		AccountInclude:      []string{},
		IncludeTransactions: boolPtr(true),
		IncludeAccounts:     boolPtr(true),
		IncludeEntries:      boolPtr(false),
	}

	slots := make(map[string]*pb.SubscribeRequestFilterSlots)
	slots["slots"] = &pb.SubscribeRequestFilterSlots{
		FilterByCommitment: boolPtr(true),
	}

	commitment := pb.CommitmentLevel_CONFIRMED
	req := &pb.SubscribeRequest{
		Blocks:     blocks,
		Slots:      slots,
		Commitment: &commitment,
	}

	log.Println("Starting block and slot monitoring...")
	log.Println("Monitoring blocks for Token Program, System Program, and Wrapped SOL activities...")

	// Connect and handle updates
	if err := manager.Connect(ctx, req); err != nil {
		log.Fatal(err)
	}

	// Keep the main goroutine running
	select {}
}
*/
